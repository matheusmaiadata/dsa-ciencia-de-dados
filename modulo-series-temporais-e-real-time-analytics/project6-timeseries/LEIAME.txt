# Modelagem de Séries Temporais e Real-Time Analytics com Apache Spark e Databricks
# Instalação e Configuração do Cluster Spark

# Abra o terminal ou prompt de comando e acesse a pasta onde estão os arquivos no seu computador

# Execute o comando abaixo para criar e inicializar o Cluster
docker compose -f docker-compose.yml up -d --scale spark-worker-dsa=2

# Dentro do container master, acesse a pasta onde estão os jupyter notebooks e execute o comando abaixo:
jupyter notebook --ip=0.0.0.0 --no-browser --allow-root

# Spark Master
http://localhost:9090

# History Server
http://localhost:18080

# Para desligar o Cluster
docker compose -f docker-compose.yml down