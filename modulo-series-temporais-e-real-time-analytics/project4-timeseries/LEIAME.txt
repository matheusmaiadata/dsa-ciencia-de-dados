# Modelagem de Séries Temporais e Real-Time Analytics com Apache Spark e Databricks
# Instalação e Configuração do Cluster Spark

# Abra o terminal ou prompt de comando e acesse a pasta onde estão os arquivos no seu computador

# Execute o comando abaixo para criar e inicializar o Cluster
docker compose -f docker-compose.yml up -d --scale spark-worker-dsa=2

# Spark Master
http://localhost:9090

# History Server
http://localhost:18080

# Para desligar o Cluster
docker compose -f docker-compose.yml down

# Comandos que devem ser executados no terminal ou prompt de comando:

# Comandos do Cluster Spark

docker exec spark-master-dsa spark-submit --deploy-mode client ./jobs/projeto4-v1.py
docker exec spark-master-dsa spark-submit --deploy-mode client ./jobs/projeto4-v2.py
docker exec spark-master-dsa spark-submit --deploy-mode client ./jobs/projeto4-v3.py